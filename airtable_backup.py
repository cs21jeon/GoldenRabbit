import os
from dotenv import load_dotenv
import json
import time
import requests
import logging
import traceback
import hashlib
from urllib.parse import urlparse
from pathlib import Path
from datetime import datetime
import schedule

dotenv_path = os.path.join(os.path.dirname(__file__), '.env')
load_dotenv(dotenv_path)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, 
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                    filename='/home/sftpuser/logs/airtable_backup.log')
logger = logging.getLogger('airtable_backup')

# ë°±ì—… ë””ë ‰í† ë¦¬ ì„¤ì • - ë‹¨ì¼ í´ë” ì‚¬ìš©
BACKUP_DIR = '/home/sftpuser/www/airtable_backup'
os.makedirs(BACKUP_DIR, exist_ok=True)

# ì—ì–´í…Œì´ë¸” ì„¤ì •
AIRTABLE_KEY = os.environ.get("AIRTABLE_API_KEY")
BASE_ID = os.environ.get("AIRTABLE_BASE_ID", "appGSg5QfDNKgFf73")
TABLE_ID = os.environ.get("AIRTABLE_TABLE_ID", "tblnR438TK52Gr0HB")

# ê° ë·° ì„¤ì •
VIEWS = {
    'all': {
        'id': os.environ.get("AIRTABLE_ALL_VIEW_ID", "viwyV15T4ihMpbDbr"),
        'filename': 'all_properties.json'
    },
    'reconstruction': {
        'id': 'viwzEVzrr47fCbDNU',  # ì¬ê±´ì¶•ìš© í† ì§€
        'filename': 'reconstruction_properties.json'
    },
    'high_yield': {
        'id': 'viwxS4dKAcQWmB0Be',  # ê³ ìˆ˜ìµë¥  ê±´ë¬¼
        'filename': 'high_yield_properties.json'
    },
    'low_cost': {
        'id': 'viwUKnawSP8SkV9Sx',  # ì €ê°€ë‹¨ë…ì£¼íƒ
        'filename': 'low_cost_properties.json'
    }
}

def calculate_data_hash(data):
    """ë°ì´í„°ì˜ í•´ì‹œê°’ì„ ê³„ì‚°í•˜ì—¬ ë³€ê²½ì‚¬í•­ ê°ì§€"""
    data_str = json.dumps(data, sort_keys=True, ensure_ascii=False)
    return hashlib.md5(data_str.encode('utf-8')).hexdigest()

def load_previous_data(filename):
    """ì´ì „ ë°±ì—… ë°ì´í„° ë¡œë“œ"""
    file_path = os.path.join(BACKUP_DIR, filename)
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"ì´ì „ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ({filename}): {e}")
    return None

def save_backup_data(data, filename):
    """ë°±ì—… ë°ì´í„° ì €ì¥"""
    file_path = os.path.join(BACKUP_DIR, filename)
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    logger.info(f"ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename}")

def compare_and_update_data(new_data, view_name, filename):
    """ë°ì´í„° ë¹„êµ í›„ ë³€ê²½ì‚¬í•­ì´ ìˆì„ ë•Œë§Œ ì—…ë°ì´íŠ¸"""
    previous_data = load_previous_data(filename)
    
    # ìƒˆ ë°ì´í„° í•´ì‹œ ê³„ì‚°
    new_hash = calculate_data_hash(new_data)
    
    # ì´ì „ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ì €ì¥
    if previous_data is None:
        logger.info(f"'{view_name}' - ì´ì „ ë°ì´í„° ì—†ìŒ, ìƒˆë¡œ ì €ì¥")
        save_backup_data(new_data, filename)
        return True, len(new_data), 0, len(new_data)
    
    # ì´ì „ ë°ì´í„° í•´ì‹œ ê³„ì‚°
    previous_hash = calculate_data_hash(previous_data)
    
    # ë°ì´í„°ê°€ ë™ì¼í•˜ë©´ ì—…ë°ì´íŠ¸ í•˜ì§€ ì•ŠìŒ
    if new_hash == previous_hash:
        logger.info(f"'{view_name}' - ë°ì´í„° ë³€ê²½ì‚¬í•­ ì—†ìŒ, ì—…ë°ì´íŠ¸ ê±´ë„ˆëœ€")
        return False, len(new_data), 0, 0
    
    # ë³€ê²½ì‚¬í•­ì´ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
    logger.info(f"'{view_name}' - ë°ì´í„° ë³€ê²½ ê°ì§€, ì—…ë°ì´íŠ¸ ì§„í–‰")
    
    # ë ˆì½”ë“œë³„ ë³€ê²½ì‚¬í•­ ë¶„ì„
    previous_records = {record.get('id'): record for record in previous_data}
    new_records = {record.get('id'): record for record in new_data}
    
    added_count = len(set(new_records.keys()) - set(previous_records.keys()))
    removed_count = len(set(previous_records.keys()) - set(new_records.keys()))
    
    modified_count = 0
    for record_id in set(new_records.keys()) & set(previous_records.keys()):
        if calculate_data_hash(new_records[record_id]) != calculate_data_hash(previous_records[record_id]):
            modified_count += 1
    
    logger.info(f"'{view_name}' ë³€ê²½ì‚¬í•­ - ì¶”ê°€: {added_count}, ì‚­ì œ: {removed_count}, ìˆ˜ì •: {modified_count}")
    
    # ìƒˆ ë°ì´í„° ì €ì¥
    save_backup_data(new_data, filename)
    
    return True, len(new_data), added_count + removed_count + modified_count, len(new_data)

def backup_airtable_data():
    """ì—ì–´í…Œì´ë¸”ì˜ ëª¨ë“  ë·° ë°ì´í„°ë¥¼ ë°±ì—… (ë³€ê²½ì‚¬í•­ë§Œ ì—…ë°ì´íŠ¸)"""
    start_time = time.time()
    logger.info(f"====== ì—ì–´í…Œì´ë¸” ë°±ì—… ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ======")
    
    if not AIRTABLE_KEY:
        logger.error("AIRTABLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
    
    headers = {
        "Authorization": f"Bearer {AIRTABLE_KEY}"
    }
    
    total_records = 0
    success_count = 0
    total_changes = 0
    updated_views = []
    all_records = []  # ëª¨ë“  ë ˆì½”ë“œ ì €ì¥ (ì´ë¯¸ì§€ ì²˜ë¦¬ìš©)
    
    # ê° ë·°ë³„ë¡œ ë°ì´í„° ë°±ì—…
    for view_name, view_info in VIEWS.items():
        view_id = view_info['id']
        filename = view_info['filename']
        
        logger.info(f"'{view_name}' ë·° ë°±ì—… ì‹œì‘ (ID: {view_id})")
        
        try:
            # ëª¨ë“  ë ˆì½”ë“œ ê°€ì ¸ì˜¤ê¸° (í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬)
            view_records = []
            offset = None
            page_count = 0
            
            while True:
                url = f"https://api.airtable.com/v0/{BASE_ID}/{TABLE_ID}"
                params = {'view': view_id}
                
                if offset:
                    params['offset'] = offset
                
                response = requests.get(url, headers=headers, params=params)
                
                if response.status_code != 200:
                    logger.error(f"API ìš”ì²­ ì‹¤íŒ¨: {response.status_code} - {response.text}")
                    break
                
                data = response.json()
                records = data.get('records', [])
                view_records.extend(records)
                
                # ì „ì²´ ë ˆì½”ë“œ ëª©ë¡ì—ë„ ì¶”ê°€ (ì´ë¯¸ì§€ ì²˜ë¦¬ìš©, all ë·°ì—ì„œë§Œ)
                if view_name == 'all':
                    all_records.extend(records)
                
                logger.info(f"  í˜ì´ì§€ {page_count + 1}: {len(records)}ê°œ ë ˆì½”ë“œ ë¡œë“œ")
                page_count += 1
                
                # ë‹¤ìŒ í˜ì´ì§€ í™•ì¸
                offset = data.get('offset')
                if not offset:
                    break
            
            # ë°ì´í„° ë¹„êµ ë° ì—…ë°ì´íŠ¸
            was_updated, record_count, changes, final_count = compare_and_update_data(
                view_records, view_name, filename
            )
            
            if was_updated:
                updated_views.append(view_name)
                total_changes += changes
            
            total_records += record_count
            success_count += 1
            
        except Exception as e:
            logger.error(f"'{view_name}' ë·° ë°±ì—… ì‹¤íŒ¨: {str(e)}")
            logger.error(traceback.format_exc())
    
    # ì´ë¯¸ì§€ ë°±ì—… (ì „ì²´ ë ˆì½”ë“œì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ, all ë·°ê°€ ì—…ë°ì´íŠ¸ëœ ê²½ìš°ì—ë§Œ)
    image_stats = {"new_images": 0, "updated_images": 0, "skipped_images": 0, "total_processed": 0}
    if 'all' in updated_views and all_records:
        logger.info("ì´ë¯¸ì§€ ë°±ì—… ì‹œì‘")
        image_stats = backup_property_images(all_records)
    else:
        logger.info("ë°ì´í„° ë³€ê²½ì‚¬í•­ì´ ì—†ì–´ ì´ë¯¸ì§€ ë°±ì—… ê±´ë„ˆëœ€")

    # ë°±ì—… ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        'last_backup_date': datetime.now().strftime('%Y-%m-%d'),
        'last_backup_time': datetime.now().isoformat(),
        'total_records': total_records,
        'views_processed': success_count,
        'total_views': len(VIEWS),
        'updated_views': updated_views,
        'total_changes': total_changes,
        'image_stats': image_stats,
        'backup_type': 'incremental'
    }
    
    metadata_path = os.path.join(BACKUP_DIR, 'metadata.json')
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    elapsed_time = time.time() - start_time
    
    if updated_views:
        logger.info(f"====== ë°±ì—… ì™„ë£Œ: {len(updated_views)}ê°œ ë·° ì—…ë°ì´íŠ¸ ({', '.join(updated_views)}), ì´ {total_changes}ê°œ ë³€ê²½ì‚¬í•­, {elapsed_time:.2f}ì´ˆ ì†Œìš” ======")
    else:
        logger.info(f"====== ë°±ì—… ì™„ë£Œ: ë³€ê²½ì‚¬í•­ ì—†ìŒ, {elapsed_time:.2f}ì´ˆ ì†Œìš” ======")
    
    return success_count == len(VIEWS)

def backup_property_images(records):
    """ë§¤ë¬¼ ì´ë¯¸ì§€ë¥¼ ë°±ì—…í•˜ëŠ” í•¨ìˆ˜ (ì¤‘ë³µ ì œê±° ë° ìµœì í™” ë²„ì „)"""
    # ì´ë¯¸ì§€ ì €ì¥ ë””ë ‰í† ë¦¬
    image_dir = os.path.join(BACKUP_DIR, 'images')
    os.makedirs(image_dir, exist_ok=True)
    
    # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    metadata_path = os.path.join(image_dir, 'image_metadata.json')
    
    # ê¸°ì¡´ ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ë¡œë“œ
    image_metadata = {}
    if os.path.exists(metadata_path):
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f:
                image_metadata = json.load(f)
        except:
            logger.error("ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
    
    new_images = 0
    updated_images = 0
    skipped_images = 0
    error_images = 0
    cleaned_duplicates = 0
    
    def get_image_priority(filename):
        """ì´ë¯¸ì§€ íŒŒì¼ ìš°ì„ ìˆœìœ„ ê²°ì •"""
        filename_lower = filename.lower()
        
        # 1ìˆœìœ„: ì›ë³¸ íŒŒì¼ëª… (ë‚ ì§œ, ì¹´ì¹´ì˜¤í†¡ ë“±)
        if any(keyword in filename_lower for keyword in ['202', 'kakao', 'img_', 'dsc_', 'photo_202']):
            return (1, len(filename))
        
        # 2ìˆœìœ„: representative íŒŒì¼
        elif 'representative' in filename_lower:
            return (2, len(filename))
        
        # 3ìˆœìœ„: ì˜ë¯¸ìˆëŠ” íŒŒì¼ëª…
        elif not filename_lower.startswith('photo_') or len(filename) > 15:
            return (3, len(filename))
        
        # 4ìˆœìœ„: photo_ ë¡œ ì‹œì‘í•˜ëŠ” ìƒì„±ëœ íŒŒì¼ëª…
        else:
            return (4, len(filename))
    
    def clean_existing_duplicates(record_image_dir, record_id):
        """ê¸°ì¡´ ì¤‘ë³µ íŒŒì¼ë“¤ ì •ë¦¬"""
        if not os.path.exists(record_image_dir):
            return None, 0
        
        # ê¸°ì¡´ ì´ë¯¸ì§€ íŒŒì¼ë“¤ ì°¾ê¸°
        existing_files = []
        for f in os.listdir(record_image_dir):
            file_path = os.path.join(record_image_dir, f)
            if (os.path.isfile(file_path) and 
                f.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp')) and
                os.path.getsize(file_path) > 1000):  # 1KB ì´ìƒë§Œ
                existing_files.append({
                    'filename': f,
                    'path': file_path,
                    'size': os.path.getsize(file_path),
                    'priority': get_image_priority(f)
                })
        
        if not existing_files:
            return None, 0
        
        # ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ì •ë ¬
        existing_files.sort(key=lambda x: (x['priority'][0], -x['size']))
        
        # ê°€ì¥ ì¢‹ì€ íŒŒì¼ ì„ íƒ
        best_file = existing_files[0]
        files_to_delete = existing_files[1:]  # ë‚˜ë¨¸ì§€ëŠ” ì‚­ì œ ëŒ€ìƒ
        
        deleted_count = 0
        for file_info in files_to_delete:
            try:
                os.remove(file_info['path'])
                logger.info(f"ì¤‘ë³µ íŒŒì¼ ì‚­ì œ: {record_id}/{file_info['filename']} (ìš°ì„ ìˆœìœ„: {file_info['priority'][0]})")
                deleted_count += 1
            except Exception as e:
                logger.warning(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {file_info['filename']} - {e}")
        
        return best_file['filename'], deleted_count
    
    for record in records:
        record_id = record.get('id')
        fields = record.get('fields', {})
        
        if not record_id:
            continue
        
        # ë ˆì½”ë“œë³„ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬
        record_image_dir = os.path.join(image_dir, record_id)
        os.makedirs(record_image_dir, exist_ok=True)
        
        # ğŸ”§ ê¸°ì¡´ ì¤‘ë³µ íŒŒì¼ë“¤ ì •ë¦¬
        existing_best_file, deleted_count = clean_existing_duplicates(record_image_dir, record_id)
        cleaned_duplicates += deleted_count
        
        # ê¸°ì¡´ì— ì¢‹ì€ íŒŒì¼ì´ ìˆìœ¼ë©´ ìƒˆë¡œ ë‹¤ìš´ë¡œë“œí•˜ì§€ ì•ŠìŒ
        if existing_best_file:
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            image_metadata[f"{record_id}_optimized"] = True
            image_metadata[f"{record_id}_filename"] = existing_best_file
            skipped_images += 1
            continue
        
        # ğŸ†• ìƒˆë¡œìš´ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë¡œì§
        image_urls = []
        processed_urls = set()  # ì¤‘ë³µ URL ë°©ì§€
        
        # ìš°ì„ ìˆœìœ„ 1: ëŒ€í‘œì‚¬ì§„ í•„ë“œ (ì›ë³¸ íŒŒì¼ëª… ìœ ì§€)
        if isinstance(fields.get('ëŒ€í‘œì‚¬ì§„'), list) and fields['ëŒ€í‘œì‚¬ì§„']:
            for i, attachment in enumerate(fields['ëŒ€í‘œì‚¬ì§„']):
                if attachment.get('url') and attachment['url'] not in processed_urls:
                    original_filename = attachment.get('filename', f'representative_{i+1}.jpg')
                    image_urls.append({
                        'url': attachment['url'],
                        'filename': original_filename,
                        'type': 'representative',
                        'priority': 1
                    })
                    processed_urls.add(attachment['url'])
        
        # ìš°ì„ ìˆœìœ„ 2: ì‚¬ì§„ë§í¬ í•„ë“œ (ëŒ€í‘œì‚¬ì§„ì— ì—†ëŠ” URLë§Œ)
        if fields.get('ì‚¬ì§„ë§í¬'):
            photo_links = fields['ì‚¬ì§„ë§í¬'].split(',')
            for i, link in enumerate(photo_links):
                link = link.strip()
                if link and link.startswith('http') and link not in processed_urls:
                    image_urls.append({
                        'url': link,
                        'filename': f'photo_link_{i+1}.jpg',
                        'type': 'link',
                        'priority': 2
                    })
                    processed_urls.add(link)
        
        # ë ˆì½”ë“œì— ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ë‹¤ìŒìœ¼ë¡œ
        if not image_urls:
            continue
        
        # ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ì²« ë²ˆì§¸ë§Œ ë‹¤ìš´ë¡œë“œ
        image_urls.sort(key=lambda x: x['priority'])
        img_info = image_urls[0]  # ê°€ì¥ ìš°ì„ ìˆœìœ„ ë†’ì€ ì´ë¯¸ì§€ë§Œ
        
        url = img_info['url']
        img_type = img_info['type']
        
        try:
            # íŒŒì¼ëª… ì²˜ë¦¬
            parsed_url = urlparse(url)
            path_parts = Path(parsed_url.path).parts
            original_filename = img_info['filename'] or path_parts[-1]
            
            # í™•ì¥ì í™•ì¸
            if '.' not in original_filename:
                original_filename += '.jpg'
            
            # íŒŒì¼ëª… ì •ë¦¬ (íŠ¹ìˆ˜ë¬¸ì ì œê±°)
            filename = "".join(c for c in original_filename if c.isalnum() or c in '.-_').strip()
            if not filename or filename == '.jpg':
                filename = f"image_{int(time.time())}.jpg"
            
            # ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
            image_path = os.path.join(record_image_dir, filename)
            
            # ì´ë¯¸ì§€ URL í•´ì‹œ ìƒì„± (ë³€ê²½ ê°ì§€ìš©)
            url_hash = hashlib.md5(url.encode()).hexdigest()
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ì´ì „ í•´ì‹œ í™•ì¸
            prev_hash = image_metadata.get(f"{record_id}_hash")
            
            # ì´ë¯¸ì§€ê°€ ì´ë¯¸ ì¡´ì¬í•˜ê³  í•´ì‹œê°€ ê°™ìœ¼ë©´ ìŠ¤í‚µ
            if os.path.exists(image_path) and prev_hash == url_hash:
                skipped_images += 1
                continue
            
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            logger.info(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {record_id} -> {filename}")
            response = requests.get(url, timeout=30, stream=True)
            
            if response.status_code == 200:
                # ì„ì‹œ íŒŒì¼ë¡œ ë¨¼ì € ë‹¤ìš´ë¡œë“œ
                temp_path = image_path + '.tmp'
                
                with open(temp_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                
                # íŒŒì¼ í¬ê¸° í™•ì¸ (ìµœì†Œ 1KB)
                if os.path.getsize(temp_path) > 1000:
                    # ì„±ê³µì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë˜ë©´ ì •ì‹ íŒŒì¼ë¡œ ì´ë™
                    os.rename(temp_path, image_path)
                    
                    # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                    image_metadata[f"{record_id}_hash"] = url_hash
                    image_metadata[f"{record_id}_filename"] = filename
                    image_metadata[f"{record_id}_type"] = img_type
                    image_metadata[f"{record_id}_optimized"] = True
                    
                    if prev_hash:
                        updated_images += 1
                        logger.info(f"âœ… ì´ë¯¸ì§€ ì—…ë°ì´íŠ¸: {filename} ({img_type})")
                    else:
                        new_images += 1
                        logger.info(f"âœ… ìƒˆ ì´ë¯¸ì§€ ì €ì¥: {filename} ({img_type})")
                else:
                    # íŒŒì¼ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ì‚­ì œ
                    os.remove(temp_path)
                    logger.warning(f"íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŒ: {url}")
                    error_images += 1
            else:
                logger.warning(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {url}, ìƒíƒœ ì½”ë“œ: {response.status_code}")
                error_images += 1
                
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {url}, ì˜¤ë¥˜: {str(e)}")
            error_images += 1
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    try:
        image_metadata['last_optimization'] = datetime.now().isoformat()
        image_metadata['optimization_stats'] = {
            'duplicates_cleaned': cleaned_duplicates,
            'new_images': new_images,
            'updated_images': updated_images
        }
        
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(image_metadata, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    logger.info(f"ğŸ‰ ì´ë¯¸ì§€ ë°±ì—… ìµœì í™” ì™„ë£Œ!")
    logger.info(f"   - ìƒˆ ì´ë¯¸ì§€: {new_images}ê°œ")
    logger.info(f"   - ì—…ë°ì´íŠ¸: {updated_images}ê°œ") 
    logger.info(f"   - ìŠ¤í‚µ: {skipped_images}ê°œ")
    logger.info(f"   - ì˜¤ë¥˜: {error_images}ê°œ")
    logger.info(f"   - ì¤‘ë³µ íŒŒì¼ ì •ë¦¬: {cleaned_duplicates}ê°œ")
    
    return {
        'new_images': new_images,
        'updated_images': updated_images,
        'skipped_images': skipped_images,
        'error_images': error_images,
        'duplicates_cleaned': cleaned_duplicates,
        'total_processed': new_images + updated_images + skipped_images + error_images,
        'optimization_enabled': True
    }

def cleanup_old_backups():
    """ì˜¤ë˜ëœ ë°±ì—… í´ë” ì •ë¦¬ (ë‚ ì§œ í˜•ì‹ í´ë”ë“¤ë§Œ)"""
    try:
        import shutil
        from datetime import datetime
        
        removed_count = 0
        for folder_name in os.listdir(BACKUP_DIR):
            folder_path = os.path.join(BACKUP_DIR, folder_name)
            
            # ë‚ ì§œ í˜•ì‹(YYYY-MM-DD) í´ë”ë§Œ ì‚­ì œ ëŒ€ìƒ
            if os.path.isdir(folder_path) and len(folder_name) == 10 and folder_name.count('-') == 2:
                try:
                    # í´ë”ëª…ì´ ë‚ ì§œ í˜•ì‹ì¸ì§€ í™•ì¸
                    datetime.strptime(folder_name, '%Y-%m-%d')
                    # ë‚ ì§œ í˜•ì‹ì´ë©´ ì‚­ì œ
                    shutil.rmtree(folder_path)
                    logger.info(f"ì˜¤ë˜ëœ ë°±ì—… í´ë” ì‚­ì œ: {folder_name}")
                    removed_count += 1
                except ValueError:
                    # ë‚ ì§œ í˜•ì‹ì´ ì•„ë‹Œ í´ë”ëŠ” ë¬´ì‹œ
                    continue
                except Exception as e:
                    logger.error(f"í´ë” ì‚­ì œ ì‹¤íŒ¨ {folder_name}: {e}")
        
        if removed_count > 0:
            logger.info(f"ì´ {removed_count}ê°œì˜ ì˜¤ë˜ëœ ë°±ì—… í´ë”ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")
        else:
            logger.info("ì •ë¦¬í•  ì˜¤ë˜ëœ ë°±ì—… í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        logger.error(f"ë°±ì—… ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

"""
def run_scheduler():
    # ì²˜ìŒ ì‹¤í–‰ ì‹œ ì˜¤ë˜ëœ ë°±ì—… í´ë” ì •ë¦¬
    cleanup_old_backups()
    
    # ë§¤ì¼ 03:00ì— ë°±ì—… ì‹¤í–‰
    schedule.every().day.at("03:00").do(backup_airtable_data)
    
    logger.info("ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ë¨ - ë§¤ì¼ 03:00ì— ë°±ì—… ì‹¤í–‰")
    
    while True:
        schedule.run_pending()
        time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ìŠ¤ì¼€ì¤„ í™•ì¸
"""
        
if __name__ == "__main__":
    # ì‹œì‘ ì‹œ ì˜¤ë˜ëœ ë°±ì—… í´ë” ì •ë¦¬
    cleanup_old_backups()
    
    # ë°±ì—… ì‹¤í–‰
    backup_airtable_data()
    
"""    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰
    run_scheduler()
"""