import os
from dotenv import load_dotenv
import json
import time
import requests
import logging
import traceback
import hashlib
from urllib.parse import urlparse
from pathlib import Path
from datetime import datetime
import shutil
import schedule

dotenv_path = os.path.join(os.path.dirname(__file__), '.env')
load_dotenv(dotenv_path)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, 
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                    filename='/home/sftpuser/logs/airtable_backup.log')
logger = logging.getLogger('airtable_backup')

# ë°±ì—… ë””ë ‰í† ë¦¬ ì„¤ì • - ë‹¨ì¼ í´ë” ì‚¬ìš©
BACKUP_DIR = '/home/sftpuser/www/airtable_backup'
os.makedirs(BACKUP_DIR, exist_ok=True)

# ì—ì–´í…Œì´ë¸” ì„¤ì •
AIRTABLE_KEY = os.environ.get("AIRTABLE_API_KEY")
BASE_ID = os.environ.get("AIRTABLE_BASE_ID", "appGSg5QfDNKgFf73")
TABLE_ID = os.environ.get("AIRTABLE_TABLE_ID", "tblnR438TK52Gr0HB")

# ê° ë·° ì„¤ì •
VIEWS = {
    'all': {
        'id': os.environ.get("AIRTABLE_ALL_VIEW_ID", "viwyV15T4ihMpbDbr"),
        'filename': 'all_properties.json'
    },
    'reconstruction': {
        'id': 'viwzEVzrr47fCbDNU',  # ì¬ê±´ì¶•ìš© í† ì§€
        'filename': 'reconstruction_properties.json'
    },
    'high_yield': {
        'id': 'viwxS4dKAcQWmB0Be',  # ê³ ìˆ˜ìµë¥  ê±´ë¬¼
        'filename': 'high_yield_properties.json'
    },
    'low_cost': {
        'id': 'viwUKnawSP8SkV9Sx',  # ì €ê°€ë‹¨ë…ì£¼íƒ
        'filename': 'low_cost_properties.json'
    }
}

# ğŸ†• ì™„ì „ ìƒˆë¡œê³ ì¹¨ ëª¨ë“œ ì„¤ì •
FULL_REFRESH_MODE = True  # Trueë¡œ ì„¤ì •í•˜ë©´ ë§¤ë²ˆ ì™„ì „ ìƒˆë¡œê³ ì¹¨

def save_backup_data(data, filename):
    """ë°±ì—… ë°ì´í„° ì €ì¥"""
    file_path = os.path.join(BACKUP_DIR, filename)
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    logger.info(f"ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename} ({len(data)}ê°œ ë ˆì½”ë“œ)")

def cleanup_image_directory():
    """ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì™„ì „ ì •ë¦¬ (ìƒˆë¡œê³ ì¹¨ ëª¨ë“œì—ì„œë§Œ)"""
    if not FULL_REFRESH_MODE:
        return
    
    image_dir = os.path.join(BACKUP_DIR, 'images')
    
    if os.path.exists(image_dir):
        try:
            # ê¸°ì¡´ ì´ë¯¸ì§€ í´ë” ì™„ì „ ì‚­ì œ
            shutil.rmtree(image_dir)
            logger.info("ğŸ—‘ï¸ ê¸°ì¡´ ì´ë¯¸ì§€ í´ë” ì™„ì „ ì‚­ì œ")
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ í´ë” ì‚­ì œ ì‹¤íŒ¨: {e}")
    
    # ìƒˆ ì´ë¯¸ì§€ í´ë” ìƒì„±
    os.makedirs(image_dir, exist_ok=True)
    logger.info("ğŸ“ ìƒˆ ì´ë¯¸ì§€ í´ë” ìƒì„±")

def backup_airtable_data():
    """ì—ì–´í…Œì´ë¸”ì˜ ëª¨ë“  ë·° ë°ì´í„°ë¥¼ ë°±ì—… (ì™„ì „ ìƒˆë¡œê³ ì¹¨ ë°©ì‹)"""
    start_time = time.time()
    
    backup_mode = "ì™„ì „ ìƒˆë¡œê³ ì¹¨" if FULL_REFRESH_MODE else "ì¦ë¶„ ì—…ë°ì´íŠ¸"
    logger.info(f"====== ì—ì–´í…Œì´ë¸” ë°±ì—… ì‹œì‘ ({backup_mode}): {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ======")
    
    if not AIRTABLE_KEY:
        logger.error("AIRTABLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
    
    headers = {
        "Authorization": f"Bearer {AIRTABLE_KEY}"
    }
    
    total_records = 0
    success_count = 0
    all_records = []  # ëª¨ë“  ë ˆì½”ë“œ ì €ì¥ (ì´ë¯¸ì§€ ì²˜ë¦¬ìš©)
    
    # ğŸ†• ì™„ì „ ìƒˆë¡œê³ ì¹¨ ëª¨ë“œì—ì„œ ì´ë¯¸ì§€ í´ë” ì •ë¦¬
    if FULL_REFRESH_MODE:
        cleanup_image_directory()
    
    # ê° ë·°ë³„ë¡œ ë°ì´í„° ë°±ì—…
    for view_name, view_info in VIEWS.items():
        view_id = view_info['id']
        filename = view_info['filename']
        
        logger.info(f"'{view_name}' ë·° ë°±ì—… ì‹œì‘ (ID: {view_id})")
        
        try:
            # ëª¨ë“  ë ˆì½”ë“œ ê°€ì ¸ì˜¤ê¸° (í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬)
            view_records = []
            offset = None
            page_count = 0
            
            while True:
                url = f"https://api.airtable.com/v0/{BASE_ID}/{TABLE_ID}"
                params = {'view': view_id}
                
                if offset:
                    params['offset'] = offset
                
                response = requests.get(url, headers=headers, params=params)
                
                if response.status_code != 200:
                    logger.error(f"API ìš”ì²­ ì‹¤íŒ¨: {response.status_code} - {response.text}")
                    break
                
                data = response.json()
                records = data.get('records', [])
                view_records.extend(records)
                
                # ì „ì²´ ë ˆì½”ë“œ ëª©ë¡ì—ë„ ì¶”ê°€ (ì´ë¯¸ì§€ ì²˜ë¦¬ìš©, all ë·°ì—ì„œë§Œ)
                if view_name == 'all':
                    all_records.extend(records)
                
                logger.info(f"  í˜ì´ì§€ {page_count + 1}: {len(records)}ê°œ ë ˆì½”ë“œ ë¡œë“œ")
                page_count += 1
                
                # ë‹¤ìŒ í˜ì´ì§€ í™•ì¸
                offset = data.get('offset')
                if not offset:
                    break
            
            # ğŸ†• ì™„ì „ ìƒˆë¡œê³ ì¹¨ ëª¨ë“œ: í•­ìƒ ì €ì¥
            if FULL_REFRESH_MODE:
                save_backup_data(view_records, filename)
                logger.info(f"âœ… '{view_name}' ë·° ì™„ì „ ìƒˆë¡œê³ ì¹¨ ì™„ë£Œ: {len(view_records)}ê°œ ë ˆì½”ë“œ")
            else:
                # ê¸°ì¡´ ì¦ë¶„ ì—…ë°ì´íŠ¸ ë¡œì§ì€ ì—¬ê¸°ì— ê·¸ëŒ€ë¡œ ìœ ì§€
                # (í•„ìš”ì‹œ ê¸°ì¡´ compare_and_update_data í•¨ìˆ˜ ì‚¬ìš©)
                save_backup_data(view_records, filename)
            
            total_records += len(view_records)
            success_count += 1
            
        except Exception as e:
            logger.error(f"'{view_name}' ë·° ë°±ì—… ì‹¤íŒ¨: {str(e)}")
            logger.error(traceback.format_exc())
    
    # ğŸ†• ì´ë¯¸ì§€ ë°±ì—… (ì™„ì „ ìƒˆë¡œê³ ì¹¨ ëª¨ë“œì—ì„œëŠ” í•­ìƒ ì‹¤í–‰)
    image_stats = {"new_images": 0, "updated_images": 0, "skipped_images": 0, "total_processed": 0}
    if all_records:  # FULL_REFRESH_MODEì—ì„œëŠ” updated_views ì¡°ê±´ ì œê±°
        logger.info("ì´ë¯¸ì§€ ë°±ì—… ì‹œì‘")
        image_stats = backup_property_images_full_refresh(all_records)
    else:
        logger.info("ë°±ì—…í•  ë ˆì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ë°±ì—… ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        'last_backup_date': datetime.now().strftime('%Y-%m-%d'),
        'last_backup_time': datetime.now().isoformat(),
        'backup_mode': backup_mode,
        'full_refresh_enabled': FULL_REFRESH_MODE,
        'total_records': total_records,
        'views_processed': success_count,
        'total_views': len(VIEWS),
        'image_stats': image_stats,
        'backup_type': 'full_refresh' if FULL_REFRESH_MODE else 'incremental'
    }
    
    metadata_path = os.path.join(BACKUP_DIR, 'metadata.json')
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    elapsed_time = time.time() - start_time
    
    logger.info(f"====== ë°±ì—… ì™„ë£Œ ({backup_mode}): ì´ {total_records}ê°œ ë ˆì½”ë“œ, {elapsed_time:.2f}ì´ˆ ì†Œìš” ======")
    
    return success_count == len(VIEWS)

def backup_property_images_full_refresh(records):
    """ë§¤ë¬¼ ì´ë¯¸ì§€ë¥¼ ë°±ì—…í•˜ëŠ” í•¨ìˆ˜ (ì™„ì „ ìƒˆë¡œê³ ì¹¨ ë²„ì „)"""
    # ì´ë¯¸ì§€ ì €ì¥ ë””ë ‰í† ë¦¬
    image_dir = os.path.join(BACKUP_DIR, 'images')
    os.makedirs(image_dir, exist_ok=True)
    
    # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ
    metadata_path = os.path.join(image_dir, 'image_metadata.json')
    
    # ğŸ†• ì™„ì „ ìƒˆë¡œê³ ì¹¨ ëª¨ë“œì—ì„œëŠ” ë©”íƒ€ë°ì´í„°ë„ ìƒˆë¡œ ì‹œì‘
    image_metadata = {
        'backup_mode': 'full_refresh',
        'backup_date': datetime.now().isoformat(),
        'total_records_processed': 0
    }
    
    new_images = 0
    error_images = 0
    
    def get_best_image_from_record(record):
        """ë ˆì½”ë“œì—ì„œ ê°€ì¥ ì¢‹ì€ ì´ë¯¸ì§€ 1ê°œ ì„ íƒ"""
        fields = record.get('fields', {})
        
        # ìš°ì„ ìˆœìœ„ 1: ëŒ€í‘œì‚¬ì§„ í•„ë“œ (ì²« ë²ˆì§¸ ì´ë¯¸ì§€)
        if isinstance(fields.get('ëŒ€í‘œì‚¬ì§„'), list) and fields['ëŒ€í‘œì‚¬ì§„']:
            attachment = fields['ëŒ€í‘œì‚¬ì§„'][0]  # ì²« ë²ˆì§¸ë§Œ
            if attachment.get('url'):
                return {
                    'url': attachment['url'],
                    'filename': attachment.get('filename', 'representative.jpg'),
                    'type': 'representative'
                }
        
        # ìš°ì„ ìˆœìœ„ 2: ì‚¬ì§„ë§í¬ í•„ë“œ (ì²« ë²ˆì§¸ ë§í¬)
        if fields.get('ì‚¬ì§„ë§í¬'):
            photo_links = fields['ì‚¬ì§„ë§í¬'].split(',')
            for link in photo_links:
                link = link.strip()
                if link and link.startswith('http'):
                    return {
                        'url': link,
                        'filename': 'photo_link.jpg',
                        'type': 'link'
                    }
        
        return None
    
    for record in records:
        record_id = record.get('id')
        
        if not record_id:
            continue
        
        # ë ˆì½”ë“œë³„ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬
        record_image_dir = os.path.join(image_dir, record_id)
        os.makedirs(record_image_dir, exist_ok=True)
        
        # ê°€ì¥ ì¢‹ì€ ì´ë¯¸ì§€ 1ê°œ ì„ íƒ
        best_image = get_best_image_from_record(record)
        
        if not best_image:
            continue
        
        url = best_image['url']
        img_type = best_image['type']
        
        try:
            # íŒŒì¼ëª… ì²˜ë¦¬
            original_filename = best_image['filename']
            
            # í™•ì¥ì í™•ì¸
            if '.' not in original_filename:
                original_filename += '.jpg'
            
            # íŒŒì¼ëª… ì •ë¦¬ (íŠ¹ìˆ˜ë¬¸ì ì œê±°)
            filename = "".join(c for c in original_filename if c.isalnum() or c in '.-_').strip()
            if not filename or filename == '.jpg':
                filename = f"image_{int(time.time())}.jpg"
            
            # ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
            image_path = os.path.join(record_image_dir, filename)
            
            # ğŸ†• í•­ìƒ ìƒˆë¡œ ë‹¤ìš´ë¡œë“œ (ì™„ì „ ìƒˆë¡œê³ ì¹¨)
            logger.info(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ: {record_id} -> {filename}")
            response = requests.get(url, timeout=30, stream=True)
            
            if response.status_code == 200:
                # ì„ì‹œ íŒŒì¼ë¡œ ë¨¼ì € ë‹¤ìš´ë¡œë“œ
                temp_path = image_path + '.tmp'
                
                with open(temp_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                
                # íŒŒì¼ í¬ê¸° í™•ì¸ (ìµœì†Œ 1KB)
                if os.path.getsize(temp_path) > 1000:
                    # ì„±ê³µì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë˜ë©´ ì •ì‹ íŒŒì¼ë¡œ ì´ë™
                    os.rename(temp_path, image_path)
                    
                    # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                    image_metadata[f"{record_id}_filename"] = filename
                    image_metadata[f"{record_id}_type"] = img_type
                    image_metadata[f"{record_id}_url"] = url
                    
                    new_images += 1
                    logger.info(f"âœ… ì´ë¯¸ì§€ ì €ì¥: {filename} ({img_type})")
                else:
                    # íŒŒì¼ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ì‚­ì œ
                    os.remove(temp_path)
                    logger.warning(f"íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŒ: {url}")
                    error_images += 1
            else:
                logger.warning(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {url}, ìƒíƒœ ì½”ë“œ: {response.status_code}")
                error_images += 1
                
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {url}, ì˜¤ë¥˜: {str(e)}")
            error_images += 1
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    try:
        image_metadata['total_records_processed'] = len(records)
        image_metadata['stats'] = {
            'new_images': new_images,
            'error_images': error_images,
            'success_rate': f"{(new_images / (new_images + error_images) * 100):.1f}%" if (new_images + error_images) > 0 else "0%"
        }
        
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(image_metadata, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    logger.info(f"ğŸ‰ ì´ë¯¸ì§€ ë°±ì—… ì™„ë£Œ (ì™„ì „ ìƒˆë¡œê³ ì¹¨)!")
    logger.info(f"   - ìƒˆ ì´ë¯¸ì§€: {new_images}ê°œ")
    logger.info(f"   - ì˜¤ë¥˜: {error_images}ê°œ")
    logger.info(f"   - ì„±ê³µë¥ : {(new_images / (new_images + error_images) * 100):.1f}%" if (new_images + error_images) > 0 else "0%")
    
    return {
        'new_images': new_images,
        'updated_images': 0,  # ì™„ì „ ìƒˆë¡œê³ ì¹¨ì—ì„œëŠ” ëª¨ë‘ ìƒˆ ì´ë¯¸ì§€
        'skipped_images': 0,
        'error_images': error_images,
        'total_processed': new_images + error_images,
        'full_refresh_mode': True
    }

def cleanup_old_backups():
    """ì˜¤ë˜ëœ ë°±ì—… í´ë” ì •ë¦¬ (ë‚ ì§œ í˜•ì‹ í´ë”ë“¤ë§Œ)"""
    try:
        removed_count = 0
        for folder_name in os.listdir(BACKUP_DIR):
            folder_path = os.path.join(BACKUP_DIR, folder_name)
            
            # ë‚ ì§œ í˜•ì‹(YYYY-MM-DD) í´ë”ë§Œ ì‚­ì œ ëŒ€ìƒ
            if os.path.isdir(folder_path) and len(folder_name) == 10 and folder_name.count('-') == 2:
                try:
                    # í´ë”ëª…ì´ ë‚ ì§œ í˜•ì‹ì¸ì§€ í™•ì¸
                    datetime.strptime(folder_name, '%Y-%m-%d')
                    # ë‚ ì§œ í˜•ì‹ì´ë©´ ì‚­ì œ
                    shutil.rmtree(folder_path)
                    logger.info(f"ì˜¤ë˜ëœ ë°±ì—… í´ë” ì‚­ì œ: {folder_name}")
                    removed_count += 1
                except ValueError:
                    # ë‚ ì§œ í˜•ì‹ì´ ì•„ë‹Œ í´ë”ëŠ” ë¬´ì‹œ
                    continue
                except Exception as e:
                    logger.error(f"í´ë” ì‚­ì œ ì‹¤íŒ¨ {folder_name}: {e}")
        
        if removed_count > 0:
            logger.info(f"ì´ {removed_count}ê°œì˜ ì˜¤ë˜ëœ ë°±ì—… í´ë”ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")
        else:
            logger.info("ì •ë¦¬í•  ì˜¤ë˜ëœ ë°±ì—… í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        logger.error(f"ë°±ì—… ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

def run_scheduler():
    """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰"""
    # ì²˜ìŒ ì‹¤í–‰ ì‹œ ì˜¤ë˜ëœ ë°±ì—… í´ë” ì •ë¦¬
    cleanup_old_backups()
    
    # ë§¤ì¼ 03:00ì— ë°±ì—… ì‹¤í–‰
    schedule.every().day.at("03:00").do(backup_airtable_data)
    
    backup_mode = "ì™„ì „ ìƒˆë¡œê³ ì¹¨" if FULL_REFRESH_MODE else "ì¦ë¶„ ì—…ë°ì´íŠ¸"
    logger.info(f"ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ë¨ ({backup_mode}) - ë§¤ì¼ 03:00ì— ë°±ì—… ì‹¤í–‰")
    
    while True:
        schedule.run_pending()
        time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ìŠ¤ì¼€ì¤„ í™•ì¸
        
if __name__ == "__main__":
    # ì‹œì‘ ì‹œ ì˜¤ë˜ëœ ë°±ì—… í´ë” ì •ë¦¬
    cleanup_old_backups()
    
    # ë°±ì—… ì‹¤í–‰
    backup_airtable_data()
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰ (ì£¼ì„ í•´ì œí•˜ë©´ í™œì„±í™”)
    # run_scheduler()